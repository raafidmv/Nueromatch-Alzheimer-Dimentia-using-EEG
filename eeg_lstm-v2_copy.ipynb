{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Classification from EEG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following analyes EEG data taken in experiments where participants where exposed to light and sound events. This code cleans that n channel EEG data and uses a long short-term memory recurrent neural network to classify the time following light or sound events by which event occured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the random seed for reproducibility\n",
    "import random\n",
    "seed=2023\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg1 and events1 are the test data from a single person\n",
    "# code assumes eeg1 and events1 are csv files in the current working directory\n",
    "event_path = ''\n",
    "eeg_path = ''\n",
    "eeg1 = pd.read_csv(eeg_path + \"\")\n",
    "new_columns = eeg1.columns.values\n",
    "print(eeg1.shape)\n",
    "\n",
    "eeg1.columns = new_columns\n",
    "\n",
    "events1 = pd.read_csv(event_path + \"\") #, delimiter=\"\\t\"\n",
    "print(events1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_col = np.arange(eeg1.shape[0])\n",
    "time_col = time_col/500\n",
    "time_col = time_col*1000\n",
    "time_col = time_col.reshape(-1, 1)\n",
    "\n",
    "eeg1.insert(0, 'time', time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eeg1.shape)\n",
    "print(eeg1.head())\n",
    "# print(eeg1.loc[(eeg1[\"time\"] > 150000) & (eeg1[\"time\"] < 160000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample of the data to ease building the model, unused in final run\n",
    "eeg1_smol = eeg1\n",
    "events1_smol = events1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_eeg(eeg_data):\n",
    "    # breaks apart an eeg dataframe, scales the eeg readings, and reassmbles it into a dataframe\n",
    "    column_list = eeg_data.columns[1:eeg1.shape[1]]\n",
    "    print(column_list)\n",
    "    time = eeg_data['time']\n",
    "    # sample = eeg_data['sample']\n",
    "    eeg_array = eeg_data[column_list]\n",
    "    eeg_stnd = scale_data(eeg_array)\n",
    "    eeg_stnd_df = pd.DataFrame(eeg_stnd, index=eeg_data.index, columns=column_list)\n",
    "    eeg_stnd = pd.concat([time, eeg_stnd_df], axis =1)\n",
    "    return eeg_stnd\n",
    "\n",
    "def scale_data(unscaled_data):\n",
    "    # helper function for standardize_eeg, fits a scaler and transforms the data \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(unscaled_data)\n",
    "    scaled_data = scaler.transform(unscaled_data)\n",
    "    return scaled_data\n",
    "# takes in eeg dataframe and event dataframe, cleans them, 1hot encodes the events\n",
    "def clean_eeg(eeg, events, event_interval_length, eeg_slice_length):\n",
    "    #event_list = []\n",
    "    array_list = [] \n",
    "    index_list = []\n",
    "    eeg = standardize_eeg(eeg) # function for standardizing the eeg readings\n",
    "    #events_new = build_zero_events(events)\n",
    "    # iterate over the rows of the events and slice out the corresponding eeg data\n",
    "    for index, row in itertools.islice(events.iterrows(), None, None): # loop through events data\n",
    "        #build_event_list(row, event_list) #\n",
    "        # print(row.latency)\n",
    "#         tmin = row\n",
    "        tmin, tmax = build_event_intervals(row, events)\n",
    "        # print(tmin, tmax)\n",
    "        eeg_slice = cut_event_intervals(eeg, tmin, tmax)\n",
    "        # print('slice')\n",
    "        array_list, index_list = build_array(eeg_slice, eeg_slice_length, \n",
    "                                             index, index_list, array_list)\n",
    "    y_int = events.iloc[index_list] # take the event types for the correct index\n",
    "    y_int = y_int['type'].values    # take just the event types as an array\n",
    "    #y_int = y_int.as_matrix()            # save the event types as a matrix\n",
    "    #y, lb = one_hot_events(y_int)        # one-hot the event types and save the binarizer\n",
    "    X = np.stack(array_list, axis = 0)   # stack the arrays so the whole thing is 3D\n",
    "    return X, y_int                     # return the data, outputs, and the binarizer\n",
    "    \n",
    "        \n",
    "def build_event_list(row, event_list):\n",
    "    # helper function to pull event types out of event data in the right order\n",
    "    event_type = getattr(row, \"type\")\n",
    "    event_list.append(event_type)\n",
    "        \n",
    "def build_event_intervals(row, events):\n",
    "    # helper function to get the time intervals associated with each event\n",
    "#     tmin = getattr(row, \"latency\")\n",
    "    stim_onset = row.latency\n",
    "    tmin = stim_onset\n",
    "    tmax = stim_onset + 512*1.5\n",
    "    return tmin, tmax\n",
    "\n",
    "def cut_event_intervals(eeg, tmin, tmax):\n",
    "    # helper function to slice up the eeg data so each slice is associated with one event\n",
    "    eeg_slice = eeg.loc[(eeg[\"time\"] > tmin) & (eeg[\"time\"] < tmax)]\n",
    "    eeg_slice.drop([\"time\"], axis = 1, inplace = True)\n",
    "    return eeg_slice\n",
    "    \n",
    "def build_array(eeg_slice, eeg_slice_length, index, index_list, array_list):\n",
    "    # helper function to build an array out of the eeg slices and pad them out to a standard length\n",
    "    if len(eeg_slice) < eeg_slice_length:\n",
    "        index_list.append(index)\n",
    "        eeg_matrix = eeg_slice.to_numpy()\n",
    "        padded_matrix = np.pad(eeg_matrix, ((0, eeg_slice_length - len(eeg_matrix)), (0,0)),\n",
    "                                   'constant', constant_values=0)\n",
    "        array_list.append(padded_matrix)\n",
    "    return array_list, index_list\n",
    "\n",
    "def one_hot_events(events):\n",
    "    # helper function for one-hot encoding the events\n",
    "    events_list = list(events)\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(events_list)\n",
    "    events_1hot = lb.transform(events_list)\n",
    "    return events_1hot, lb\n",
    "\n",
    "def invert_one_hot(events, lb):\n",
    "    # function for decoding one-hot, binarizer made in one_hot_events\n",
    "    inv_events = lb.inverse_transform(events)\n",
    "    return inv_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset parameters\n",
    "\n",
    "# define model parameters\n",
    "samples = events1.shape[0]  # how many trials of eeg data\n",
    "n_features = eeg1.shape[1] - 1  # how many channels of eeg in each sample\n",
    "time_steps = ... # how many ms was each sample run for\n",
    "event_types = ... #len(set(y))  # how many different event types (light, sound, etc) are there # 6 large, 4 smol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data into useable form and store as X and y\n",
    "X, y = clean_eeg(eeg1, events1, samples, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes the minor event types. There were only a couple hundred examples of each, whereas the used events had a \n",
    "# couple thousand examples\n",
    "remove_list = [1]              # designate unwanted event types\n",
    "drop_list = np.isin(y, remove_list)    # create a list of indices associated with unwanted events                  \n",
    "drop_array = np.array(drop_list)       # make the list of indices to drop into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make X, y's with the unwanted events removed\n",
    "y_short_int = y[np.isin(y,remove_list, invert=True)]\n",
    "X_short = X[np.isin(y, remove_list, invert=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_short.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the y data without the unwanted events\n",
    "y_short, lb = one_hot_events(y_short_int) \n",
    "# print(y_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# use strat. shuffle split to get indices for test and training data \n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=seed)\n",
    "sss.get_n_splits(X_short, y_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the indices generated by stratified shuffle split and make the test and training datasets\n",
    "for train_index, test_index in sss.split(X_short, y_short):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_short[train_index], X_short[test_index]\n",
    "    y_train, y_test = y_short[train_index], y_short[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "from keras.utils import to_categorical\n",
    "temp = to_categorical(y_train, 3)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# code for building an LSTM with 100 neurons and dropout. Runs for 50 epochs\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(1024, return_sequences=False, input_shape=(time_steps, n_features)))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "#model.add(LSTM(100)) dramatically worse results\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, batch_size=16, epochs=50)\n",
    "# score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "class LossAccHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('accuracy'))\n",
    "        self.draw_plot()\n",
    "\n",
    "    def draw_plot(self):\n",
    "        epochs = len(self.losses)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        if(epochs % 50 == 0):\n",
    "            # Plot loss\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(range(1, epochs + 1), self.losses, label='Training Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "\n",
    "            # Plot accuracy\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(range(1, epochs + 1), self.accuracies, label='Training Accuracy')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Initialize the callback\n",
    "history = LossAccHistory()\n",
    "\n",
    "# Add ProgbarLogger to display real-time progress\n",
    "progbar = keras.callbacks.ProgbarLogger()\n",
    "\n",
    "# Fit the model with the callbacks\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=500, callbacks=[history, progbar])\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "\n",
    "# Plot the final training history\n",
    "history.draw_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saved model details\n",
    "standardized\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(2, output_dim=256))\n",
    "model.add(LSTM(100, input_shape=(time_steps, n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=50)\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "This model run for 50 epochs had:\n",
    "\n",
    "* binary crossentropy 0.41922928811677918\n",
    "\n",
    "* accuracy 0.8529411764705882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
